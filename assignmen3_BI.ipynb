{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('music_genre.csv')\n",
    "\n",
    "\n",
    "# a. Attribute types and their semantics\n",
    "attribute_types = df.dtypes\n",
    "semantics = df.describe(include='all')\n",
    "print('Attribute Types: \\n',  attribute_types)\n",
    "print('Semantics: \\n', semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d7df490b51d5f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# b. Statistical properties and correlations\n",
    "# Select only numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Displaying correlation matrix for numeric columns\n",
    "correlations_numeric = numeric_columns.corr()\n",
    "\n",
    "# Visualization of correlations using a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlations_numeric, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix for Numeric Columns')\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4387a07048a6c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c. Data quality aspects\n",
    "missing_values = df.isnull().sum()\n",
    "data_distribution = df['music_genre'].value_counts()\n",
    "print( 'Missing values: \\n', missing_values)\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Plot the distribution of missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_percentage.plot(kind='bar')\n",
    "plt.title('Distribution of Missing Values in Each Column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Percentage of Missing Values')\n",
    "plt.savefig('distribution_missing_values.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57ff482be279dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d. Visual exploration of data properties and hypotheses\n",
    "# Visualize correlations using a heatmap (the same heatmap as was in part b. )\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlations_numeric, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05bc4935a5159d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize data distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='music_genre', data=df)\n",
    "plt.title('Data Distribution of Music Genres')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('data_distribution_music_genres.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37d94642f1e46a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e. Evaluate potentially ethically sensitive attributes and unbalanced distributions\n",
    "# No etchically sensitive attributes, the classes of the dataset are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62f73e5f16cbe8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f. Potential risks and biases\n",
    "# It's important to consult domain experts for understanding biases related to music genres, as for example regarding the change in music genre for the same artist, which is described in the report.\n",
    "\n",
    "# g. Actions likely required in data preparation\n",
    "# Clean missing values \n",
    "# Analyse the outliers and remove them if needed\n",
    "# Consider the correlated attributes during the preprocessing\n",
    "# Remove the instance id\n",
    "# Check if the same artist has songs for different genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11bb015a208fc4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3.a Analyze and perform necessary actions based on analysis performed in the Data Understanding phase.\n",
    "numerical_attributes = ['popularity' , 'acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "                            'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
    "\n",
    "\n",
    "    # 3.a.1 set attribute tempo to float (already done for task d)\n",
    "df = pd.read_csv('music_genre.csv')\n",
    "df['tempo'] = pd.to_numeric(df['tempo'], errors='coerce')  # 'coerce' will turn non-numeric values to NaN\n",
    "\n",
    "    # 3.a.2 Clean missing values (task a)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "    # 3.a.3 Remove the instance id (task a)\n",
    "df_cleaned = df_cleaned.drop(columns=['instance_id'])\n",
    "\n",
    "    # 3.a.4 filter out duration of -1 (4.340 rows) (already done for task d)\n",
    "df_cleaned = df_cleaned[df_cleaned['duration_ms'] != -1.0]\n",
    "\n",
    "\n",
    "    # 3.a.5 Analyze outliers and remove them if needed (done on numerical attributes) (task a)\n",
    "for attribute in numerical_attributes:\n",
    "   q_low = df_cleaned[attribute].quantile(0.01)\n",
    "   q_high = df_cleaned[attribute].quantile(0.99)\n",
    "   df_cleaned_outliers = df_cleaned[(df_cleaned[attribute] > q_low) & (df_cleaned[attribute] < q_high)]\n",
    "\n",
    "    # 3.a.6 #filter out artist \"empty_field\" (1.934 rows) (already done for task d)\n",
    "df_cleaned_outliers = df_cleaned_outliers[df_cleaned_outliers['artist_name'] != 'empty_field']\n",
    "\n",
    "\n",
    "    # 3.a.7 Check if the same artist has songs for more than two genres (3.166 rows) (task a)\n",
    "artist_genre_counts = df_cleaned_outliers.groupby('artist_name')['music_genre'].nunique()\n",
    "artists_with_multiple_genres = artist_genre_counts[artist_genre_counts > 2].index.tolist()\n",
    "\n",
    "for artist in artists_with_multiple_genres:\n",
    "    df_cleaned_outliers = df_cleaned_outliers[df_cleaned_outliers['artist_name'] != artist]\n",
    "\n",
    "# Consider the correlated attributes during the preprocessing (task a, but is done during task b)\n",
    "print('filtered rows:')\n",
    "print(len(df)-len(df_cleaned_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22699b53-eee0-484a-aa02-6bebc1e0e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.b Analyze options and potential for derived attributes (note: if the potential is considered low, \n",
    "#these obviously do not necessarily have to be applied for your analysis, but options should be documented)\n",
    "\n",
    "# 3.b.1 Feature Scaling (for numerical data)\n",
    "    # Example: Scaling numerical features to a standard range (e.g., Min-Max scaling).\n",
    "    # Potential: Ensures that features contribute equally to the model and improves convergence.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_derrived = df_cleaned_outliers.copy()\n",
    "df_derrived[numerical_attributes] = scaler.fit_transform(df_derrived[numerical_attributes])\n",
    "\n",
    "# 3.b.2 Consider correlated attributes during preprocessing\n",
    "    # Attribute creation\n",
    "    # Example: Combining two or more features to create an interaction term (e.g., loudness * energy).\n",
    "    # Potential: Captures relationships between features that may have a combined effect on the target variable.\n",
    "\n",
    "\n",
    "corr_matrix = df_cleaned_outliers.select_dtypes(include=[np.number]).corr().abs()\n",
    "most_correlated_pairs = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    "                    k=1).astype(bool)).stack().sort_values(ascending=False)).head(3).reset_index()\n",
    "\n",
    "df_derrived = df_derrived.copy()\n",
    "correlated_attributes = []\n",
    "\n",
    "for index, row in most_correlated_pairs.iterrows():\n",
    "    new_column_name = f\"{row['level_0']}_{row['level_1']}_interaction\"\n",
    "    correlated_attributes.append(new_column_name)\n",
    "    df_derrived[new_column_name] = df_derrived[row['level_0']] * df_derrived[row['level_1']]\n",
    "\n",
    "\n",
    "df_derrived[correlated_attributes] = scaler.fit_transform(df_derrived[correlated_attributes])\n",
    "\n",
    "\n",
    "\n",
    "# 3.b.3 Categorical Feature Encoding:\n",
    "    # Example: One-hot encoding categorical variables (e.g., genre) to convert them into numerical format.\n",
    "    # Potential: Enables the use of categorical data in machine learning models\n",
    "\n",
    "categorical_features = ['key', 'mode']  \n",
    "df_derrived = pd.get_dummies(df_derrived, columns=categorical_features)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_derrived\n",
    "\n",
    "#NOT DONE, low potential!\n",
    "# 3.b.4 Ratios and Proportions:\n",
    "    # Example: Creating ratios between two numerical features (e.g., energy divided by loudness).\n",
    "    # Potential: Highlights relative importance or relationships between features.\n",
    "\n",
    "# 3.b.5 Moving Averages:\n",
    "    # Example: Calculating the moving average of a numerical feature over a specified window.\n",
    "    # Potential: Smoothens trends and captures changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab83bbf-a179-4701-a8e6-dea28107f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.c Analyze options for additional external data sources, attributes that might be useful to better\n",
    "    # address the business objectives or data mining goals (Note: this description may be\n",
    "    # hypothetical, i.e. you are not necessarily required to actually obtain and integrate the external\n",
    "    # data for the analysis)\n",
    "\n",
    "    #not needed, because we have all necessary data in our data set (hypothetical on our platform)\n",
    "    # 3.c.1 User Preferences:\n",
    "    # Source: User surveys, feedback forms, or interaction logs.\n",
    "    # Attributes: Explicit user preferences, liked genres, and feedback on existing recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac14e9-bed3-49d3-92e7-aaf40db481fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.d Describe other pre-processing steps considered, specifying which ones were applied or not\n",
    "    # applied due to which reason. (e.g. data cleansing, transformations, binning, scaling, outlier\n",
    "    # removal, attribute removal, transcoding, â€¦) at a level of detail that ensures reproducibility of\n",
    "    # changes to the data.\n",
    "\n",
    "    # 3.a.1 transformation of attribute to float\n",
    "\n",
    "    # 3.a.3 attribute removal (instance id)\n",
    "        # Applied: Remove instance id - irrelevant or redundant attributes that do not contribute significantly to the analysis or model.\n",
    "        # Reason: Reduces dimensionality, focuses on essential features, and may improve model performance.\n",
    "    \n",
    "    # 3.a.4 data cleansing\n",
    "    \n",
    "    # 3.a.5 outlier removal\n",
    "\n",
    "    # 3.a.6 data cleansing\n",
    "\n",
    "    # 3.a.7 data cleansing\n",
    "\n",
    "    # 3.b.1 feature scaling\n",
    "    # 3.b.2 attribute creation\n",
    "    # 3.b.3 feature encoding\n",
    "\n",
    "\n",
    "#not applied: \n",
    "\n",
    "# Binning:\n",
    "    # Not Applied: Did not bin numerical values into discrete intervals.\n",
    "    # Reason: Depending on the nature of the data, binning may or may not be suitable. In some cases, it can lead to loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b7af7d59b23d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4.Training and evaluating the SVM model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Splitting the target variable\n",
    "y = df_derrived['music_genre']\n",
    "\n",
    "# Features (dropping the useless ones)\n",
    "X = df_derrived.drop(['obtained_date', 'artist_name', 'track_name', 'music_genre'], axis=1)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Defining a range of C values for hyperparameter tuning\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Creating the SVM classifier using quadratic kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=2)\n",
    "\n",
    "# Performing grid search with cross-validation for receiving the best parameter for C\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, return_train_score=True, error_score='raise')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieveing results from the grid search\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Displaing all accuracies for each C value\n",
    "print(\"All accuracies for each C value:\")\n",
    "for index, row in cv_results.iterrows():\n",
    "    print(f\"C = {row['param_C']:.3f}: Train Accuracy = {row['mean_train_score']:.3f}, Validation Accuracy = {row['mean_test_score']:.3f}\")\n",
    "\n",
    "# Get the best C value\n",
    "best_C = grid_search.best_params_['C']\n",
    "print(f'\\nBest C value: {best_C}')\n",
    "\n",
    "# Use the best C value to create the final SVM model\n",
    "final_svm_classifier = SVC(kernel='poly',degree=2, C=best_C)\n",
    "final_svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the validation set\n",
    "y_val_pred = final_svm_classifier.predict(X_val)\n",
    "\n",
    "# Performance on the validation set\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "classification_rep_val = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print('\\nValidation Set Performance:')\n",
    "print(f'Accuracy: {accuracy_val:.2f}')\n",
    "print('Classification Report:\\n', classification_rep_val)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = final_svm_classifier.predict(X_test)\n",
    "\n",
    "# Performance on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "\n",
    "# Calculating confusion matrix\n",
    "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plotting confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=final_rf_classifier.classes_,\n",
    "            yticklabels=final_rf_classifier.classes_)\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix_SVM.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification report for the test set\n",
    "classification_rep_test = classification_report(y_test, y_test_pred)\n",
    "print('\\nTest Set Performance:')\n",
    "print(f'Accuracy: {accuracy_test:.2f}')\n",
    "print('Classification Report:\\n', classification_rep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162130c01e31667",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4.Training and evaluating the Random Forest model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Splitting the target variable\n",
    "y = df_derrived['music_genre']\n",
    "\n",
    "# Features (dropping the useless features)\n",
    "X = df_derrived.drop(['obtained_date', 'artist_name', 'track_name', 'music_genre'], axis=1)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets with stratification\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Defining a range of parameters for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 25, 50, 75],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 50, 500],\n",
    "    'min_samples_leaf': [2, 4, 40, 100]\n",
    "}\n",
    "\n",
    "# Creating the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieving the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print('Best Parameters:', best_params)\n",
    "\n",
    "# Using the best parameters to create the final Random Forest model\n",
    "final_rf_classifier = RandomForestClassifier(random_state=42, **best_params)\n",
    "final_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the validation set\n",
    "y_val_pred = final_rf_classifier.predict(X_val)\n",
    "\n",
    "# Performance on the validation set\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "classification_rep_val = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print('\\nValidation Set Performance:')\n",
    "print(f'Accuracy: {accuracy_val:.2f}')\n",
    "print('Classification Report:\\n', classification_rep_val)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = final_rf_classifier.predict(X_test)\n",
    "\n",
    "# Performance on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "\n",
    "# Calculating confusion matrix\n",
    "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plotting confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=final_rf_classifier.classes_,\n",
    "            yticklabels=final_rf_classifier.classes_)\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix_random_forest.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification report for the test set\n",
    "classification_rep_test = classification_report(y_test, y_test_pred)\n",
    "print('\\nTest Set Performance:')\n",
    "print(f'Accuracy: {accuracy_test:.2f}')\n",
    "print('Classification Report:\\n', classification_rep_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66184a8d18b3b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
